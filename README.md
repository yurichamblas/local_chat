# ğŸ§  Local Chat con Qwen3 + Chainlit

**Local Chat** es un asistente conversacional basado en LLMs que funciona completamente en local, capaz de listar, leer, buscar y resumir archivos desde tu sistema de archivos, usando un modelo de lenguaje como `Qwen3:8b` (Ollama).

## âœ¨ Funcionalidades principales

- ğŸ“‚ **Listar directorios** locales accesibles.
- ğŸ“„ **Leer archivos** de texto (`.txt`, `.md`, `.json`, `.py`), PDF, Word (`.docx`), Excel (`.xlsx`, `.xls`).
- ğŸ” **Buscar texto** dentro de archivos en carpetas.
- ğŸ§¾ **Resumir archivos individuales**.
- ğŸ—‚ **Resumir automÃ¡ticamente todos los archivos compatibles** en una carpeta.
- ğŸ’¬ **Interfaz amigable** usando [Chainlit](https://docs.chainlit.io/).

## ğŸš€ CÃ³mo usar

### 1. Instala dependencias
```bash
pip install -r requirements.txt
````

### 2. Activa tu entorno virtual (opcional pero recomendado)

```bash
venv\Scripts\activate    # Windows
source venv/bin/activate # macOS/Linux
```

### 3. AsegÃºrate que Ollama estÃ© corriendo con el modelo `Qwen3:8b`

```bash
ollama run qwen3:8b
```

### 4. Ejecuta la app Chainlit

```bash
chainlit run chainlit_app.py --port 8000
```

> Puedes editar la ruta base y otros parÃ¡metros en `config.py`.

## ğŸ“ Carpeta base usada

La aplicaciÃ³n opera Ãºnicamente sobre el directorio definido en `config.py` a
travÃ©s de la variable `BASE_PATH`. ModifÃ­calo segÃºn la ruta que prefieras
antes de iniciar la app.

## ğŸ§  Modelo LLM usado

El modelo local utilizado es:

* `Qwen3:8b` (ejecutado vÃ­a [Ollama](https://ollama.com))

## ğŸ§° TecnologÃ­as

* [Chainlit](https://docs.chainlit.io/) â€” interfaz de chat local.
* [LangChain](https://www.langchain.com/) â€” agente y herramientas.
* [Ollama](https://ollama.com) â€” ejecuciÃ³n local de LLMs.
* Python: `pdfplumber`, `python-docx`, `pandas`, `openpyxl`.

---

Â¡Ya estÃ¡s listo para usar tu propio copiloto IA local! ğŸ¤–
