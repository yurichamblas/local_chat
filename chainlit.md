## 🧠 Local Chat con Qwen3 y acceso local 🔍📂

¡Bienvenido a tu asistente local con capacidades LLM! Este entorno usa **Chainlit** como interfaz conversacional y un modelo **Qwen3:8b** ejecutado localmente mediante **Ollama**, con soporte para explorar y procesar archivos directamente desde tu sistema.

### 🚀 ¿Qué puede hacer Local Chat?

- 📁 **Listar carpetas**: Muestra el contenido de directorios locales permitidos.
- 📄 **Leer archivos**: Visualiza archivos `.txt`, `.md`, `.py`, `.json`, `.pdf`, `.docx`, `.xlsx` y más.
- 🔎 **Buscar texto**: Busca una palabra o frase dentro de múltiples archivos en una carpeta.
- 📝 **Resumir documentos**: Usa LLMs para generar resúmenes breves de archivos individuales o carpetas completas.

### 🛠️ Tecnologías utilizadas

- 🧠 **Modelo:** `qwen3:8b` (Ollama)
- 💬 **Interfaz:** `Chainlit`
- 🧰 **Frameworks:** `LangChain`, `LangChain-Ollama`
- 🗂️ **Exploración local:** Integración segura y restringida con el sistema de archivos


¡Explora tus datos con IA local, sin depender de la nube! ☁️❌  
